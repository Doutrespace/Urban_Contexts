{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5819c74-d5c0-4ed4-a501-8e1024d5a67f",
   "metadata": {},
   "source": [
    "# Detection of Spatial and Functional Signatures for Environmental Measurements\n",
    "\n",
    "This Jupyter Notebook serves as the computational framework accompanying the paper, **\"Urban Contexts: A Geospatial Approach to Identifying In-Situ Measurement Sites for Urban Acoustic Environments\"**. It provides reproducible workflows designed to analyze urban spatial and functional characteristics comprehensively, enabling researchers and planners to identify contextually significant urban areas for environmental and acoustic measurements. The methodologies integrate geospatial and statistical analyses, emphasizing the connection between urban form, function, and environmental variability.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Develop Robust Methodologies for Urban Spatial Analysis**  \n",
    "   Establish a detailed framework for evaluating urban form through morphometric indicators such as dimension, shape, intensity, and connectivity. These metrics form the foundation for understanding spatial patterns and variability within urban environments.\n",
    "\n",
    "2. **Create Scalable Workflows for Extracting Urban Form and Function Metrics**  \n",
    "   Utilize scalable and reproducible geospatial workflows to integrate functional data, including Points of Interest (POIs), land use, and demographic information. These workflows enable the analysis of urban activity and accessibility in diverse contexts.\n",
    "\n",
    "3. **Develop Signature Types Based on Morphometric Variables**  \n",
    "   Employ advanced clustering techniques, such as Gaussian Mixture Models (GMM), to classify urban areas into distinct signature types. These classifications capture the interplay between form and function, providing actionable insights for urban analysis and environmental measurement.\n",
    "\n",
    "This notebook aims to provide a structured approach to urban research, offering tools for analyzing spatial and functional dynamics while addressing the challenges of variability and complexity in urban landscapes.\n",
    "\n",
    "Acknowledgments\n",
    "\n",
    "This analysis makes use of the **Momepy** Python library for urban morphology analysis. For detailed documentation and further insights, visit [Momepy Documentation](https://docs.momepy.org/en/stable/).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Analysis of Form](#1-analysis-of-form)\n",
    "   1. [Data Retrieval](#11-data-retrieval)\n",
    "      1. [Building Footprints](#111-building-footprints)\n",
    "      2. [Street Network](#112-street-network)\n",
    "      3. [Spatial Barriers](#113-spatial-barriers)\n",
    "   2. [Data Pre-Processing](#12-data-pre-processing)\n",
    "      1. [Footprints Checks and Cleaning](#121-footprints-checks-and-cleaning)\n",
    "      2. [Barriers Checks and Cleaning](#122-barriers-checks-and-cleaning)\n",
    "   3. [Generation of Geographies](#13-generation-of-geographies)\n",
    "      1. [Enclosures](#131-enclosures)\n",
    "      2. [Enclosed Tessellation](#132-enclosed-tessellation)\n",
    "   4. [Morphometric Analysis](#14-morphometric-analysis)\n",
    "      1. [Primary Morphometric Characters](#141-primary-morphometric-characters)\n",
    "      2. [Contextualization](#142-contextualization)\n",
    "2. [Analysis of Function](#2-analysis-of-function)\n",
    "   1. [Land Use and Population Density](#21-land-use-and-population-density)\n",
    "   2. [Entertainment](#22-entertainment)\n",
    "   3. [Culture](#23-culture)\n",
    "   4. [Education](#24-education)\n",
    "   5. [Health](#25-health)\n",
    "   6. [Transportation](#26-transportation)\n",
    "   7. [Green & Blue Spaces](#27-green--blue-spaces)\n",
    "   8. [Trees](#28-trees)\n",
    "   9. [NDVI](#29-ndvi)\n",
    "3. [Cluster Analysis](#3-cluster-analysis)\n",
    "   1. [Form](#31-form)\n",
    "   2. [Function](#32-function)\n",
    "   3. [Spatial Signatures](#33-spatial-signatures)\n",
    "4. [Figures](#4-figures)\n",
    "5. [Cluster Summary](#5-cluster-summary)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad2e66-8a3b-4de9-9cc0-9ba70b43c143",
   "metadata": {},
   "source": [
    "## 1. Analysis of Form\n",
    "### 1.1 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7cd1e-15e2-426d-9242-ea16e60693ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipyparallel\n",
    "import sys\n",
    "sys.path.append(\"/your system path/\")\n",
    "import momepy_utils\n",
    "import tobler\n",
    "import warnings\n",
    "import geopandas as gdp\n",
    "import libpysal\n",
    "import momepy\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import osmnx as ox\n",
    "from pandana.loaders import osm\n",
    "from shapely.geometry import Point\n",
    "from clustergram import Clustergram\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f34a34-8c91-4e67-9420-d88433a0ffdf",
   "metadata": {},
   "source": [
    "#### 1.1.1 Building Footprints\n",
    "Description and Python code related to building footprints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f1a0d-9d56-4515-ae37-67420a7e2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify a folder for storage\n",
    "folder = \"/your_storage_hd/\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd624b-6638-42e7-812f-268a11915c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify your city\n",
    "#We can use ``OSMnx`` to quickly download data from OpenStreetMap. If you intend to download larger areas, we recommend using ``pyrosm`` instead.\n",
    "place = 'WÃ¼rzburg'\n",
    "local_crs = 32634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b82c1-0d10-4e01-bbd4-cde4754783b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = osmnx.geometries.geometries_from_place(place, tags={'building':True})\n",
    "buildings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85471e05-70ac-4c11-9ddd-9bfd7b05d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bounding box of the buildings\n",
    "bbox = buildings.total_bounds\n",
    "print('Bounding box:', bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83331029-b123-48e7-8394-758804696241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning geometries\n",
    "buildings.geom_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46ee70-96d8-4f8c-a67d-f4f44b211cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-project the data from WGS84 to the local projection in meters\n",
    "buildings = buildings[buildings.geom_type == \"Polygon\"].reset_index(drop=True)\n",
    "buildings = buildings[[\"geometry\"]].to_crs(local_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80e143-9209-4030-9c18-f74559e017ec",
   "metadata": {},
   "source": [
    "#### 1.1.2 Street Network\n",
    "Description and Python code related to the street network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18f776-aef8-40b6-bf33-696861630a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the street network\n",
    "osm_graph = osmnx.graph_from_place(place, network_type='drive')\n",
    "osm_graph = osmnx.projection.project_graph(osm_graph, to_crs=local_crs)\n",
    "roads = osmnx.graph_to_gdfs(\n",
    "    osm_graph, \n",
    "    nodes=False, \n",
    "    edges=True,\n",
    "    node_geometry=False, \n",
    "    fill_edge_geometry=True\n",
    ")\n",
    "roads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2d5c5-d80f-4e96-9270-d14191f06db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads = momepy.remove_false_nodes(roads)\n",
    "roads = roads[[\"geometry\"]]\n",
    "roads[\"nID\"] = range(len(roads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15c14b-bdcf-433d-901e-40f64be05b88",
   "metadata": {},
   "source": [
    "#### 1.1.3 Spatial Barriers\n",
    "Description and Python code related to spatial barriers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dab82a-4f83-47f2-aebe-3f1ffd2985ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload aoi\n",
    "admin = gpd.read_file(folder + \"/UA_UrbanCore.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d73472-ebf8-4e6a-940f-20cd1720a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = admin.to_crs(4326).total_bounds\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34052e7-0da8-4a80-8820-fd06583cdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using `OSMnx` we can specify OSM tags, selecting which geometries should be downloaded. First we get water-related barriers.\n",
    "tags = {'natural': ['water', 'coastline']}\n",
    "\n",
    "water = ox.geometries_from_bbox(bounds[3], bounds[1], bounds[2], bounds[0], tags)\n",
    "water = water.to_crs(streets.crs)\n",
    "water[['natural', 'geometry']].to_parquet(folder + \"/parquets/water.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1873aa-3a02-4c3c-aa8b-24ee1d38f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we get railway and filter out proper geometry type (we want LineStrings representing railway tracks) and remove tunnels and trams (those are not spatial barriers).\n",
    "tags = {'railway': True}\n",
    "railway = ox.geometries_from_bbox(bounds[3], bounds[1], bounds[2], bounds[0], tags)\n",
    "railway = railway.to_crs(streets.crs)\n",
    "railway = railway[railway.geom_type == 'LineString']\n",
    "railway = railway[railway.tunnel != 'yes']\n",
    "railway = railway[~railway.railway.isin(['miniature', 'tram'])]\n",
    "railway[['railway', 'geometry']].to_parquet(folder + \"/parquets/railway.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfb33e-9abd-4fde-9c86-ebffcc653074",
   "metadata": {},
   "source": [
    "### 1.2 Data Pre-Processing\n",
    "#### 1.2.1 Footprints Checks and Cleaning\n",
    "Description and Python code for checking and cleaning building footprints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029c44a-d21b-4f0a-884f-45fe729d761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then use `momepy.CheckTessellationInput()` class to check for potential issues which may arise during enclosed tessellation.\n",
    "check = momepy.CheckTessellationInput(buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dff6fe-68c0-430c-8952-70b02ad8c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = momepy.CheckTessellationInput(buildings)\n",
    "#We see that some buildings would collapse (disappear) and some would be split, which may result in multipolygon tessellation cells.\n",
    "check.collapse.area.max()\n",
    "#If the maximum area of collapsed feature is low. we can safely remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a5200-d3bb-4f65-8f69-1ba8b4565912",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0eab1c-0185-4cc3-be4c-f68b36c92354",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings[\"uID\"] = range(len(buildings))\n",
    "buildings.to_parquet(folder + \"parquets/buildings.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1c180-1bae-4136-b7f0-f344ed88eb92",
   "metadata": {},
   "source": [
    "#### 1.2.2 Barriers Checks and Cleaning\n",
    "Description and Python code for checking and cleaning spatial barriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42aa19-e41f-4579-823d-c5687e17a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin.geom_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b96f36-da26-41fa-ad77-94483d670fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "railway.geom_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb9c84-3575-4d05-adde-43341d212996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roads seems to be fine, similarly to railway. The only step we do is extension of railway lines to snap to roads.\n",
    "extended_railway = momepy.extend_lines(railway, 30, target=streets, extension=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9702ddc-ff7f-480b-bc1a-ef44bc4de3fd",
   "metadata": {},
   "source": [
    "### 1.3 Generation of Geographies\n",
    "\n",
    "#### 1.3.1 Enclosures\n",
    "Enclosures require spatial barriers, which are roads and railway (we use the extended one we did above), limited by administrative boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5c267-bf6b-4f92-ab99-8167f0b1bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "enclosures = utils.momepy.enclosures(roads, limit=admin.iloc[[0]], additional_barriers=[extended_railway])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655c7af-f632-4cda-b97b-66b9265c976b",
   "metadata": {},
   "source": [
    "#### 1.3.2 Enclosed Tessellation\n",
    "Description and Python code for generating enclosed tessellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce30c33-4e68-4988-bb4f-65bea9d37056",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = gpd.read_parquet(folder + \"parquets/buildings.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f45957-ad65-4795-ac98-137f01a81003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tess = utils.momepy.Tessellation(buildings, 'uID', enclosures=enclosures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397a1c6-070f-4af9-94d5-3abaac966f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess.tessellation.to_parquet(folder + \"tessellation.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbabae4-f9d8-4649-b6cd-19bb7ae0f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosures.to_parquet(folder + \"enclosures.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d73bc-1494-455e-beb9-185c2127eed7",
   "metadata": {},
   "source": [
    "### 1.4 Morphometric Analysis\n",
    "\n",
    "#### 1.4.1 Primary Morphometric Characters\n",
    "Morphometric analysis begins with measuring primary characteristics, which are subsequently contextualized to reflect the tendencies and relationships within the local context of each tessellation cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534d4b5-1b0d-4bc0-9b5c-4c4d7eec7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "blg = gpd.read_parquet(folder + \"buildings.pq\")\n",
    "streets = roads\n",
    "tess = gpd.read_parquet(folder + \"tessellation.pq\")\n",
    "enclosures = gpd.read_parquet(folder + \"enclosures.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cc0a7-2f69-4fb2-8856-edf2f4535f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = tess.rename_geometry(\"tessellation\").merge(\n",
    "    blg[[\"uID\", \"geometry\"]].rename_geometry(\"buildings\"), on=\"uID\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5bf11-6b72-4d6e-a291-d7f0a8bd6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess['tID'] = range(len(tess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37a291-30d8-4332-9b45-a487126558c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0253f-eba1-4ead-ae66-d35dbde52cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "blg = tess.set_geometry('buildings').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410599b-fe00-4749-8c48-b2b3900b92af",
   "metadata": {},
   "source": [
    "At this stage, we proceed to measure the morphometric characters. For detailed information on each metric, refer to the Momepy documentation. The results for each character are appended as new columns in the dataset. https://docs.momepy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e165e-ec66-45ae-8abe-4a28d1bc2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time blg['sdbAre'] = momepy.Area(blg).series\n",
    "%time blg['sdbPer'] = momepy.Perimeter(blg).series\n",
    "%time blg['sdbCoA'] = momepy.CourtyardArea(blg, 'sdbAre').series\n",
    "\n",
    "%time blg['ssbCCo'] = momepy.CircularCompactness(blg, 'sdbAre').series\n",
    "%time blg['ssbCor'] = momepy.Corners(blg).series\n",
    "%time blg['ssbSqu'] = momepy.Squareness(blg).series\n",
    "%time blg['ssbERI'] = momepy.EquivalentRectangularIndex(blg, 'sdbAre', 'sdbPer').series\n",
    "%time blg['ssbElo'] = momepy.Elongation(blg).series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4dc803-97a5-4845-81b8-2fcc5d59fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cencon = momepy.CentroidCorners(blg)\n",
    "blg['ssbCCM'] = cencon.mean\n",
    "blg['ssbCCD'] = cencon.std\n",
    "\n",
    "%time blg['stbOri'] = momepy.Orientation(blg).series\n",
    " \n",
    "%time tess['stcOri'] = momepy.Orientation(tess).series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb8ae8-f16a-4c28-ba51-ba6e2a01b0d0",
   "metadata": {},
   "source": [
    "Next, we need to incorporate building orientation into the full dataframe alongside the tessellation data. To achieve this, we merge the datasets back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010bedaa-4b54-42ee-9c1a-213ce2da2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = tess.merge(blg[['tID', 'stbOri']], on='tID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7bc7c-585d-43b6-a3f6-cac828ae3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tess['stbCeA'] = (tess['stbOri'] - tess['stcOri']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b15a2-a446-4f80-9761-2b54e014bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tess['sdcLAL'] = momepy.LongestAxisLength(tess).series\n",
    "%time tess['sdcAre'] = momepy.Area(tess).series\n",
    "%time tess['sscCCo'] = momepy.CircularCompactness(tess, 'sdcAre').series\n",
    "%time tess['sscERI'] = momepy.EquivalentRectangularIndex(tess, 'sdcAre').series\n",
    "\n",
    "%time tess['sicCAR'] = tess.buildings.area / tess['sdcAre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc6ac4-7c2b-4c6f-87ed-ac70436732fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time blg[\"mtbSWR\"] = momepy.SharedWallsRatio(blg).series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03274123-72c0-4d68-be17-4e4c98c9fcc7",
   "metadata": {},
   "source": [
    "Certain morphometric characters require spatial weights matrices for their computation. We can generate a Queen contiguity matrix based on the enclosed tessellation to capture the spatial relationships between tessellation cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037951bb-bb90-4884-9948-ddf9286d6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time queen_1 = libpysal.weights.contiguity.Queen.from_dataframe(tess, ids=\"tID\", geom_col='tessellation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6425cc1-0678-4025-bd6a-2c9fd18f9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tess[\"mtbAli\"] = momepy.Alignment(tess.set_geometry(\"buildings\"), queen_1, \"tID\", \"stbOri\").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab223c5-8806-469d-843b-c251d24a82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tess[\"mtbNDi\"] = utils.momepy.NeighborDistance(tess.set_geometry(\"buildings\"), queen_1, \"tID\").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0506d81-5a16-4135-afec-2acc9188db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tess[\"mtcWNe\"] = momepy.Neighbors(tess, queen_1, \"tID\", weighted=True).series\n",
    "%time tess[\"mdcAre\"] = momepy.CoveredArea(tess, queen_1, \"tID\").series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ac0cd-8007-4efd-b4fb-1bb03109fb99",
   "metadata": {},
   "source": [
    "In certain scenarios, capturing the contiguity of buildings is essential. To achieve this, we can generate a Queen contiguity matrix based on the building layer, which considers shared edges and vertices between adjacent buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e923a61-1116-4d1c-9fa5-f97acccc4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time blg_q1 = libpysal.weights.contiguity.Queen.from_dataframe(blg, geom_col='buildings', silence_warnings=True)\n",
    " \n",
    "%time blg[\"ldbPWL\"] = momepy.PerimeterWall(blg, blg_q1).series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70bc05-0c8b-44aa-a0cf-73da3e4c501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time blg[\"libNCo\"] = utils.momepy.Courtyards(blg, spatial_weights=blg_q1).series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13968d-9792-4338-a03a-d5fd12cd2dda",
   "metadata": {},
   "source": [
    "Morphometric characters often necessitate defining an immediate context for analysis. In our methodology, we utilize inclusive third-order contiguity, which extends the analysis to include not only directly adjacent units but also those within three levels of contiguity. This can be efficiently generated using the initial Queen contiguity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf3fbc-180b-4eea-96f4-3288ea2000bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time queen_3 = momepy.sw_high(k=3, weights=queen_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7b489-656c-490a-a18b-d358b049702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tess['ltbIBD'] = utils.momepy.MeanInterbuildingDistance(tess.set_geometry('buildings'), queen_1, 'tID', queen_3).series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0796142-a15d-4f5e-be40-13f0e7e75af9",
   "metadata": {},
   "source": [
    "To associate tessellation cells with streets, we perform an intersection operation. This process ensures that each tessellation cell is linked to the corresponding street segments it intersects. In cases where a tessellation cell intersects multiple street segments, we calculate the intersection ratio to proportionally distribute the relationship across these segments. This approach maintains spatial consistency and ensures accurate assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44365644-6b6f-400f-806d-6ceb948e98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time links = momepy.get_network_ratio(tess, streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e566b2-8824-482c-8677-ccc886517008",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess[['edgeID_keys', 'edgeID_values']] = links\n",
    "#From these ratios we can get the primary link (the one which intersects the most)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cee255-0c0a-49c4-94f9-a6cb91b3d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = tess.edgeID_values.apply(lambda a: np.argmax(a))\n",
    "tess['edgeID_primary'] = [inds[i] for inds, i in zip(tess.edgeID_keys, keys)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fb410-4620-4ed2-bf94-9eb1fe8e88d9",
   "metadata": {},
   "source": [
    "To ensure that the links between tessellation cells and street segments are available for both tessellation and building layers, we merge the results into the building dataset. This step integrates the spatial relationships into a unified framework, enabling consistent analysis across both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637266b-3317-4c3b-9a0c-34ee467b4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "blg = blg.merge(tess[['tID', 'edgeID_primary']], on='tID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37d1dd-9ca3-4271-b30b-811a621043a0",
   "metadata": {},
   "source": [
    "To ensure that the initial integer index, used as edgeID, is preserved as an attribute of the streets, we add it explicitly to the street dataset. This precaution ensures that the edgeID remains intact, even if the data is shuffled or reordered later in the analysis. By preserving the edgeID as a dedicated column, we maintain a consistent reference for all subsequent operations and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918e93c-6d6e-45cc-a246-3538c9757216",
   "metadata": {},
   "outputs": [],
   "source": [
    "streets['edgeID_primary'] = range(len(streets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e674762-d4b8-4c51-9100-436b82868ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally we are able to measure the characters combining multiple data sources.\n",
    "%time streets[\"sdsLen\"] = momepy.Perimeter(streets).series\n",
    "%time tess[\"stcSAl\"] = momepy.StreetAlignment(tess, streets, \"stcOri\", \"edgeID_primary\").series\n",
    "%time blg[\"stbSAl\"] = momepy.StreetAlignment(blg, streets, \"stbOri\", \"edgeID_primary\").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743f407-377b-4712-bf22-3b720a5a5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time profile = momepy.StreetProfile(streets, blg, distance=3)\n",
    "streets[\"sdsSPW\"] = profile.w\n",
    "streets[\"sdsSPO\"] = profile.o\n",
    "streets[\"sdsSWD\"] = profile.wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a7a2c-8fce-4369-b074-53810ace1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time streets[\"sssLin\"] = momepy.Linearity(streets).series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d273206-91fd-421b-93d8-08a039d9ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Area Covered by each edge\n",
    "\n",
    "vals = {x:[] for x in range(len(streets))}\n",
    "for i, keys in enumerate(tess.edgeID_keys):\n",
    "    for k in keys:\n",
    "        vals[k].append(i)\n",
    "area_sums = []\n",
    "for inds in vals.values():\n",
    "    area_sums.append(tess.sdcAre.iloc[inds].sum())\n",
    "streets['sdsAre'] = area_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b4b9f-8568-4d2b-85bc-09550dc179ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Buildings per meter\n",
    "\n",
    "bpm = []\n",
    "for inds, l in zip(vals.values(), streets.sdsLen):\n",
    "    bpm.append(tess.buildings.iloc[inds].notna().sum() / l if len(inds) > 0 else 0)\n",
    "streets['sisBpM'] = bpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92807561-5401-49a7-a7e7-35099e0ef2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_q1 = libpysal.weights.contiguity.Queen.from_dataframe(\n",
    "    streets, silence_warnings=True\n",
    ")\n",
    "\n",
    "streets[\"misRea\"] = momepy.Reached(\n",
    "    streets,\n",
    "    tess,\n",
    "    \"edgeID_primary\",\n",
    "    \"edgeID_primary\",\n",
    "    spatial_weights=str_q1,\n",
    "    mode=\"count\",\n",
    ").series\n",
    "\n",
    "streets[\"mdsAre\"] = momepy.Reached(\n",
    "    streets,\n",
    "    tess,\n",
    "    \"edgeID_primary\",\n",
    "    \"edgeID_primary\",\n",
    "    spatial_weights=str_q1,\n",
    "    mode=\"sum\",\n",
    ").series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf898f-b2a1-43d8-806c-5c42088143d1",
   "metadata": {},
   "source": [
    "Next, we create a graph representation and compute connectivity characteristics. The following cell constructs a networkX.MultiGraph, calculates connectivity metrics, and outputs two GeoDataFramesâone for the original segments and another for the nodesâalong with spatial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e2edc-090f-49d0-9229-9605da4640c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time graph = momepy.gdf_to_nx(streets)\n",
    " \n",
    "print(\"node degree\")\n",
    "graph = momepy.node_degree(graph)\n",
    " \n",
    "print(\"subgraph\")\n",
    "graph = momepy.subgraph(\n",
    "    graph,\n",
    "    radius=5,\n",
    "    meshedness=True,\n",
    "    cds_length=False,\n",
    "    mode=\"sum\",\n",
    "    degree=\"degree\",\n",
    "    length=\"mm_len\",\n",
    "    mean_node_degree=False,\n",
    "    proportion={0: True, 3: True, 4: True},\n",
    "    cyclomatic=False,\n",
    "    edge_node_ratio=False,\n",
    "    gamma=False,\n",
    "    local_closeness=True,\n",
    "    closeness_weight=\"mm_len\",\n",
    ")\n",
    "print(\"cds length\")\n",
    "graph = momepy.cds_length(graph, radius=3, name=\"ldsCDL\")\n",
    " \n",
    "print(\"clustering\")\n",
    "graph = momepy.clustering(graph, name=\"xcnSCl\")\n",
    " \n",
    "print(\"mean_node_dist\")\n",
    "graph = momepy.mean_node_dist(graph, name=\"mtdMDi\")\n",
    " \n",
    "%time nodes, edges, sw = momepy.nx_to_gdf(graph, spatial_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef944c6a-72ff-4e59-8dfc-c71e50329570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time edges_w3 = momepy.sw_high(k=3, gdf=edges)\n",
    "%time edges[\"ldsMSL\"] = momepy.SegmentsLength(edges, spatial_weights=edges_w3, mean=True).series\n",
    " \n",
    "%time nodes_w5 = momepy.sw_high(k=5, weights=sw)\n",
    "%time nodes[\"lddNDe\"] = momepy.NodeDensity(nodes, edges, nodes_w5).series\n",
    "nodes[\"linWID\"] = momepy.NodeDensity(\n",
    "    nodes, edges, nodes_w5, weighted=True, node_degree=\"degree\"\n",
    ").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5aa50f-6a8f-45a5-9b1d-772e1d1db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosures[\"ldeAre\"] = momepy.Area(enclosures).series\n",
    "enclosures[\"ldePer\"] = momepy.Perimeter(enclosures).series\n",
    "enclosures[\"lseCCo\"] = momepy.CircularCompactness(enclosures, \"ldeAre\").series\n",
    "enclosures[\"lseERI\"] = momepy.EquivalentRectangularIndex(enclosures, \"ldeAre\", \"ldePer\").series\n",
    "enclosures[\"lseCWA\"] = momepy.CompactnessWeightedAxis(enclosures, \"ldeAre\", \"ldePer\").series\n",
    "enclosures[\"lteOri\"] = momepy.Orientation(enclosures).series\n",
    " \n",
    "blo_q1 = libpysal.weights.contiguity.Queen.from_dataframe(enclosures, ids=\"eID\")\n",
    " \n",
    "inp, res = enclosures.sindex.query_bulk(enclosures.geometry, predicate='intersects')\n",
    "indices, counts = np.unique(inp, return_counts=True)\n",
    "enclosures['neighbors'] = counts - 1\n",
    "enclosures['lteWNB'] = enclosures['neighbors'] / enclosures['ldePer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de68ce-c9ef-40f0-bca4-5fa5324fcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure weighted cells within enclosure\n",
    "encl_counts = tess.groupby('eID').count()\n",
    "merged = enclosures[['eID', 'ldeAre']].merge(encl_counts[['tessellation']], how='left', on='eID')\n",
    "enclosures['lieWCe'] = merged['tessellation'] / merged['ldeAre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339805c-fd2a-4e26-869b-d90b64b169db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess['ltcWRE'] = momepy.BlocksCount(tess, 'eID', queen_3, 'tID').series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a40755-1b0f-4e20-8880-15ecd2a9d9a7",
   "metadata": {},
   "source": [
    "To link the data currently associated with nodes to the tessellation, we need to determine the nearest network-based node ID for each tessellation cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0fef4-4994-4fba-b516-f9d5d546eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get node id\n",
    "%time links = momepy.get_network_ratio(tess, edges)\n",
    "tess[['edgeID_keys2', 'edgeID_values2']] = links\n",
    "%time tess['nodeID'] = momepy.get_node_id(tess, nodes, edges, node_id='nodeID', edge_keys='edgeID_keys2', edge_values='edgeID_values2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717d6a0-2172-42ec-bbf9-e241eadf3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nodes[\"sddAre\"] = momepy.Reached(\n",
    "    nodes, tess, \"nodeID\", \"nodeID\", mode=\"sum\", values=\"sdcAre\"\n",
    ").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627dd6a-5e84-4969-89fe-3d1a485f21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we link all Dataframnes together\n",
    "tess.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39dc11-8865-4233-98c7-1521bc48c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "blg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25556bc1-ec23-409b-996a-20f04fc493c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e36392-00f7-4d7a-ba24-e7fd2baa5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d7cb0-c0b5-4a47-99f1-b174bdee1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6788dc-f313-4853-8d9c-165dcec985b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tess.merge(\n",
    "    blg[\n",
    "        [\n",
    "            \"tID\",\n",
    "            \"sdbAre\",\n",
    "            \"sdbPer\",\n",
    "            \"sdbCoA\",\n",
    "            \"ssbCCo\",\n",
    "            \"ssbCor\",\n",
    "            \"ssbSqu\",\n",
    "            \"ssbERI\",\n",
    "            \"ssbElo\",\n",
    "            \"ssbCCM\",\n",
    "            \"ssbCCD\",\n",
    "            \"mtbSWR\",\n",
    "            \"ldbPWL\",\n",
    "            \"stbSAl\",\n",
    "            \"libNCo\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"tID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "data = data.merge(\n",
    "    edges[\n",
    "        [\n",
    "            \"sdsLen\",\n",
    "            \"sdsSPW\",\n",
    "            \"sdsSPO\",\n",
    "            \"sdsSWD\",\n",
    "            \"sssLin\",\n",
    "            \"sdsAre\",\n",
    "            \"sisBpM\",\n",
    "            \"misRea\",\n",
    "            \"mdsAre\",\n",
    "            \"ldsMSL\",\n",
    "            \"edgeID_primary\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"edgeID_primary\",\n",
    "    how=\"left\",\n",
    ")\n",
    "data = data.merge(\n",
    "    nodes[\n",
    "        [\n",
    "            \"degree\",\n",
    "            \"meshedness\",\n",
    "            \"proportion_3\",\n",
    "            \"proportion_4\",\n",
    "            \"proportion_0\",\n",
    "            \"local_closeness\",\n",
    "            \"ldsCDL\",\n",
    "            \"xcnSCl\",\n",
    "            \"mtdMDi\",\n",
    "            \"nodeID\",\n",
    "            \"lddNDe\",\n",
    "            \"linWID\",\n",
    "            \"sddAre\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"nodeID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "data = data.merge(\n",
    "    enclosures[\n",
    "        [\n",
    "            \"eID\",\n",
    "            \"ldeAre\",\n",
    "            \"ldePer\",\n",
    "            \"lseCCo\",\n",
    "            \"lseERI\",\n",
    "            \"lseCWA\",\n",
    "            \"lteOri\",\n",
    "            \"lteWNB\",\n",
    "            \"lieWCe\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"eID\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37242c0-26ba-4101-854e-e04d1953df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadb337-93cc-4fe1-8a6b-df251928b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(folder + \"data.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc352f2-da33-45ea-bce7-e2dbd449a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess.to_parquet(folder + \"tess.pq\")\n",
    "blg.to_parquet(folder + \"blg.pq\")\n",
    "nodes.to_parquet(folder + \"nodes.pq\")\n",
    "edges.to_parquet(folder + \"edges.pq\")\n",
    "enclosures.to_parquet(folder + \"enclosures.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff77d93-731e-4c18-a105-47c50b952446",
   "metadata": {},
   "source": [
    "#### 1.4.2 Contextualization\n",
    "Cluster analysis must be efficient for large datasets, so it avoids spatial constraints, as such algorithms typically do not scale well. However, our focus is on understanding the tendencies of characters within specific areas. Most primary characters are inherently local, offering minimal or no contextual insight. To address this, we contextualize each primary character by calculating the first, second, and third quartiles of the value distribution within an inclusive 10th-order contiguity around each tessellation cell, weighted by the inverse distance between the centroids of the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd50550-353a-4dd2-abdd-4a6a5da479d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_parquet(folder + \"data.pq\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9272ddd-842a-432e-add5-98c721c41212",
   "metadata": {},
   "source": [
    "Next, we specify the columns that exclusively represent the characters, excluding any other attributes such as geometry or IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bc62a-3ca8-4519-8436-72f4c11ed22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\n",
    "    \"stcOri\",\n",
    "    \"stbOri\",\n",
    "    \"stbCeA\",\n",
    "    \"sdcLAL\",\n",
    "    \"sdcAre\",\n",
    "    \"sscCCo\",\n",
    "    \"sscERI\",\n",
    "    \"sicCAR\",\n",
    "    \"mtbAli\",\n",
    "    \"mtbNDi\",\n",
    "    \"mtcWNe\",\n",
    "    \"mdcAre\",\n",
    "    \"ltbIBD\",\n",
    "    \"stcSAl\",\n",
    "    \"ltcWRE\",\n",
    "    \"sdbAre\",\n",
    "    \"sdbPer\",\n",
    "    \"sdbCoA\",\n",
    "    \"ssbCCo\",\n",
    "    \"ssbCor\",\n",
    "    \"ssbSqu\",\n",
    "    \"ssbERI\",\n",
    "    \"ssbElo\",\n",
    "    \"ssbCCM\",\n",
    "    \"ssbCCD\",\n",
    "    \"mtbSWR\",\n",
    "    \"ldbPWL\",\n",
    "    \"stbSAl\",\n",
    "    \"libNCo\",\n",
    "    \"sdsLen\",\n",
    "    \"sdsSPW\",\n",
    "    \"sdsSPO\",\n",
    "    \"sdsSWD\",\n",
    "    \"sssLin\",\n",
    "    \"sdsAre\",\n",
    "    \"sisBpM\",\n",
    "    \"misRea\",\n",
    "    \"mdsAre\",\n",
    "    \"ldsMSL\",\n",
    "    \"degree\",\n",
    "    \"meshedness\",\n",
    "    \"proportion_3\",\n",
    "    \"proportion_4\",\n",
    "    \"proportion_0\",\n",
    "    \"local_closeness\",\n",
    "    \"ldsCDL\",\n",
    "    \"xcnSCl\",\n",
    "    \"mtdMDi\",\n",
    "    \"lddNDe\",\n",
    "    \"linWID\",\n",
    "    \"sddAre\",\n",
    "    \"ldeAre\",\n",
    "    \"ldePer\",\n",
    "    \"lseCCo\",\n",
    "    \"lseERI\",\n",
    "    \"lseCWA\",\n",
    "    \"lteOri\",\n",
    "    \"lteWNB\",\n",
    "    \"lieWCe\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca595532-26d3-4231-be1d-bb79b9e144cc",
   "metadata": {},
   "source": [
    "We prepare a GeoDataFrame (gdf) that includes the morphometric characters alongside tessellation centroids as the geometry. Additionally, we generate a spatial weights matrix (W) representing the inclusive 10th order of contiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae789d-c337-4453-ac41-ceafebc9498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(data[characters], geometry=data.tessellation.centroid)\n",
    "%time W = momepy.sw_high(k=10, weights=libpysal.weights.Queen.from_dataframe(data, geom_col='tessellation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1292cc7-d043-4111-8b45-d8f53312cfe5",
   "metadata": {},
   "source": [
    "Next, we iterate through the GeoDataFrame to calculate the contextualization for each character, capturing the first, second, and third quartiles of the value distributions within the inclusive 10th order of contiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e6b3b-b716-4023-a64a-accd11ae757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutions = {}\n",
    "for c in characters:\n",
    "    convolutions[c] = []\n",
    "\n",
    "# measure convolutions\n",
    "for i, geom in tqdm(gdf.geometry.iteritems(), total=data.shape[0]):\n",
    "    neighbours = W.neighbors[i]\n",
    "    vicinity = gdf.iloc[neighbours]\n",
    "    distance = vicinity.distance(geom)\n",
    "    distance_decay = 1 / distance\n",
    "    \n",
    "    for c in characters:\n",
    "        values = vicinity[c].values\n",
    "        sorter = np.argsort(values)\n",
    "        values = values[sorter]\n",
    "        nan_mask = np.isnan(values)\n",
    "        if nan_mask.all():\n",
    "            convolutions[c].append(np.array([np.nan] * 3))\n",
    "        else:\n",
    "            sample_weight = distance_decay.values[sorter][~nan_mask]\n",
    "            weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "            weighted_quantiles /= np.sum(sample_weight)\n",
    "            interpolate = np.interp([.25, .5, .75], weighted_quantiles, values[~nan_mask])\n",
    "            convolutions[c].append(interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e7593-c1d7-4099-99c4-66bf72981237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resulting in a dictionary which can be exploited\n",
    "%time conv = pd.DataFrame(convolutions, index=data.index)\n",
    "%time exploded = pd.concat([pd.DataFrame(conv[c].to_list(), columns=[c + '_q1', c + '_q2',c + '_q3']) for c in characters], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c119000-4d5f-4f95-80df-b8632aabb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded.index = data.tID\n",
    "exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fd0d6-4ee1-47e8-a84c-94f03791dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded.to_parquet(folder + \"convolutions.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71c6f7-34b6-4bfb-b073-6ccee601e85e",
   "metadata": {},
   "source": [
    "## 2. Analysis of Function\n",
    "\n",
    "### 2.1 Land Use and Population Density\n",
    "Description and Python code for analyzing land use and population density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f576d-1f40-41d4-ba9b-b76899e6f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def process_geospatial_data(aoi_path, land_use_path, output_crs=32634):\n",
    "    \"\"\"\n",
    "    Generalized function to process geospatial data for an area of interest and land use.\n",
    "    \n",
    "    Parameters:\n",
    "        aoi_path (str): File path to the AOI (Area of Interest) GeoPackage.\n",
    "        land_use_path (str): File path to the land use GeoPackage.\n",
    "        output_crs (int): EPSG code for the desired Coordinate Reference System. Default is 32634.\n",
    "    \n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Processed land use GeoDataFrame clipped to AOI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        aoi_gdf = gpd.read_file(aoi_path)\n",
    "        land_use_gdf = gpd.read_file(land_use_path)\n",
    "\n",
    "        # Coordinate Reference System Transformation\n",
    "        aoi_gdf = aoi_gdf.to_crs(epsg=output_crs)\n",
    "        land_use_gdf = land_use_gdf.to_crs(epsg=output_crs)\n",
    "\n",
    "        # Clip and fix geometries\n",
    "        land_use_gdf = gpd.clip(land_use_gdf, aoi_gdf)\n",
    "        land_use_gdf.geometry = land_use_gdf.geometry.buffer(0)\n",
    "\n",
    "        return land_use_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    aoi_path = input(\"Enter the file path for the AOI GeoPackage: \").strip()\n",
    "    land_use_path = input(\"Enter the file path for the land use GeoPackage: \").strip()\n",
    "    \n",
    "    # Call the function\n",
    "    result = process_geospatial_data(aoi_path, land_use_path)\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"Data processed successfully!\")\n",
    "        # Save the result if needed\n",
    "        save_path = input(\"Enter the file path to save the clipped land use GeoDataFrame: \").strip()\n",
    "        result.to_file(save_path, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(\"Processing failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246a873-3038-4b6f-bea1-223b627539ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function from tobler to ease the process\n",
    "def area_max(source_df, target_df, variables):\n",
    "    \"\"\"\n",
    "    Join attributes from source based on the largest intersection. In case of a tie it picks the first one.   \n",
    "    \"\"\"    \n",
    "    target_df = target_df.copy()\n",
    "    target_ix, source_ix = source_df.sindex.query_bulk(target_df.geometry, predicate='intersects')\n",
    "    areas = target_df.geometry.values[target_ix].intersection(source_df.geometry.values[source_ix]).area\n",
    "\n",
    "    main = []\n",
    "    for i in range(len(target_df)):\n",
    "        mask = target_ix == i\n",
    "        if np.any(mask):\n",
    "            main.append(source_ix[mask][np.argmax(areas[mask])])\n",
    "        else:\n",
    "            main.append(np.nan)\n",
    "    \n",
    "    main = np.array(main)\n",
    "    mask = ~np.isnan(main)\n",
    "    if pd.api.types.is_list_like(variables):\n",
    "        for v in variables:\n",
    "            arr = np.empty(len(main), dtype=object)\n",
    "            arr[:] = np.nan\n",
    "            arr[mask] = source_df[v].values[main[mask].astype(int)]\n",
    "            target_df[v] = arr\n",
    "    else:\n",
    "        arr = np.empty(len(main), dtype=object)\n",
    "        arr[:] = np.nan\n",
    "        arr[mask] = source_df[variables].values[main[mask].astype(int)]\n",
    "        target_df[variables] = arr\n",
    "        \n",
    "    return target_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fa363-e343-4d06-9fb6-da149e642f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "func_data = area_max(land_use, func_data, ['class_2018','Pop2018'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437cd1e-891f-4894-a33a-deb89b62e8c6",
   "metadata": {},
   "source": [
    "### 2.2 Points of Interest (POI)\n",
    "\n",
    "This section focuses on the analysis of Points of Interest (POIs) to understand the functional characteristics of urban areas. POIs are categorized into specific types to provide insights into different urban functionalities.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.1 Culture\n",
    "\n",
    "**Description**:  \n",
    "Analyzing cultural POIs, such as theaters, libraries, and cinemas, to understand the distribution of cultural landmarks in urban areas.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.2 Education\n",
    "\n",
    "**Description**:  \n",
    "Analyzing educational institutions, including schools, universities, and kindergartens, to map and evaluate educational accessibility.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.3 Health\n",
    "\n",
    "**Description**:  \n",
    "Analyzing health services, such as hospitals, clinics, and pharmacies, to assess the spatial distribution of healthcare facilities.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.4 Transportation\n",
    "\n",
    "**Description**:  \n",
    "Analyzing transportation hubs, including bus stations and bicycle rentals, to evaluate accessibility and mobility in urban areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3f4e7-26f2-4732-8775-f26b2d1bfd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder and create it if it doesn't exist\n",
    "output_folder = \"output_amenities/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the city and categories\n",
    "city = \"WÃ¼rzburg, Germany\"  # Update as needed\n",
    "categories = {\n",
    "    'entertainment': [\"bar\", \"cafe\", \"nightclub\", \"restaurant\"],\n",
    "    'culture': [\"place_of_worship\", \"theatre\", \"library\", \"cinema\"],\n",
    "    'education': [\"school\", \"university\", \"library\", \"college\", \"kindergarten\", \"language_school\", \"music_school\", \"driving_school\", \"research_institute\", \"public_bookcase\"],\n",
    "    'health': [\"hospital\", \"clinic\", \"dentist\", \"doctors\", \"pharmacy\", \"nursing_home\", \"veterinary\", \"social_facility\"],\n",
    "    'transportation': [\"bus_station\", \"ferry_terminal\", \"taxi\", \"car_rental\", \"bicycle_rental\", \"parking\", \"fuel\"]\n",
    "}\n",
    "\n",
    "# Function to process amenities by category\n",
    "def process_amenities(category, tags, city, output_folder):\n",
    "    \"\"\"\n",
    "    Processes and saves amenities for a given category.\n",
    "    \n",
    "    Parameters:\n",
    "        category (str): The category name (e.g., 'entertainment').\n",
    "        tags (list): List of OSM tags for the category.\n",
    "        city (str): The city/place for downloading data.\n",
    "        output_folder (str): Path to save the output GeoPackage.\n",
    "    \"\"\"\n",
    "    print(f\"Processing category: {category}\")\n",
    "    amenities_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    for tag in tags:\n",
    "        try:\n",
    "            gdf = ox.geometries_from_place(city, tags={\"amenity\": tag})\n",
    "            amenities_gdf = pd.concat([amenities_gdf, gdf], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag '{tag}' in category '{category}': {e}\")\n",
    "    \n",
    "    # Filter points only\n",
    "    amenities_gdf = amenities_gdf[amenities_gdf['geometry'].apply(lambda x: isinstance(x, Point))]\n",
    "    amenities_gdf[\"latitude\"] = amenities_gdf[\"geometry\"].y\n",
    "    amenities_gdf[\"longitude\"] = amenities_gdf[\"geometry\"].x\n",
    "    \n",
    "    # Save to GeoPackage\n",
    "    output_path = os.path.join(output_folder, f\"{category}.gpkg\")\n",
    "    amenities_gdf.to_file(output_path, driver='GPKG')\n",
    "    print(f\"Saved {category} amenities to {output_path}\")\n",
    "\n",
    "# Process each category separately\n",
    "for category, tags in categories.items():\n",
    "    process_amenities(category, tags, city, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f138b-22d7-4f06-bab5-e3bc4c277445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial join to link amenities to tessellation cells\n",
    "tessellation = gpd.read_file(\"tessellation_cells.gpkg\")\n",
    "amenities = gpd.read_file(f\"{output_folder}entertainment.gpkg\")\n",
    "\n",
    "joined = gpd.sjoin(tessellation, amenities, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# Aggregating amenities per tessellation\n",
    "result = joined.groupby('tessellation_id').size().reset_index(name='amenity_count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c97b3-f8b5-44af-980e-deff0e0299c2",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.2.5 Green & Blue Spaces\n",
    "\n",
    "**Description**:  \n",
    "Analyzing urban green and blue spaces, including parks, lakes, and rivers, to assess ecological and recreational resources in the city.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.6 Trees\n",
    "\n",
    "**Description**:  \n",
    "Analyzing urban trees to evaluate canopy coverage and biodiversity in urban landscapes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.7 NDVI & LST\n",
    "\n",
    "**Description**:  \n",
    "Analyzing the Normalized Difference Vegetation Index (NDVI) to assess vegetation health and density using satellite imagery.\n",
    "\n",
    "#### 2.2.7 NOISE\n",
    "\n",
    "#### 2.2.8 Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a44bf8-cf5d-4028-8057-3ae458da8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WALKING_SPEED_M_PER_MIN = 83.3333  # Average walking speed in meters per minute (5 km/h)\n",
    "POINT_INTERVAL = 100  # Interval in meters for placing points along polygon boundaries\n",
    "\n",
    "def generate_boundary_points(polygon, interval):\n",
    "    \"\"\"\n",
    "    Generate points at specified intervals along the boundary of a polygon.\n",
    "    \n",
    "    Args:\n",
    "        polygon (Polygon or MultiPolygon): The geometry to generate points for.\n",
    "        interval (int): Distance interval for generating points.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Point objects along the boundary.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    if isinstance(polygon, MultiPolygon):\n",
    "        for part in polygon.geoms:\n",
    "            points.extend(generate_boundary_points(part, interval))\n",
    "    elif isinstance(polygon, Polygon):\n",
    "        num_points = int(np.ceil(polygon.length / interval))\n",
    "        points = [polygon.boundary.interpolate(i / num_points, normalized=True) for i in range(num_points)]\n",
    "    return points\n",
    "\n",
    "def download_and_process_amenity(category, tags, city, output_folder, interval=POINT_INTERVAL):\n",
    "    \"\"\"\n",
    "    Download and process green/blue space amenities, generating boundary points.\n",
    "    \n",
    "    Args:\n",
    "        category (str): Category name for the amenity (e.g., 'parks', 'water').\n",
    "        tags (dict): Tags for querying amenities from OpenStreetMap.\n",
    "        city (str): Name of the city to query.\n",
    "        output_folder (str): Path to save processed files.\n",
    "        interval (int): Interval in meters for placing boundary points.\n",
    "        \n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: GeoDataFrame containing generated points.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {category} data for {city}...\")\n",
    "    gdf = ox.geometries_from_place(city, tags=tags)\n",
    "    gdf = gdf[gdf.geometry.type.isin(['Polygon', 'MultiPolygon'])]\n",
    "    \n",
    "    # Generate boundary points\n",
    "    boundary_points = gdf['geometry'].apply(lambda x: generate_boundary_points(x, interval)).explode()\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=boundary_points, crs=gdf.crs)\n",
    "    \n",
    "    # Save the results\n",
    "    output_path = os.path.join(output_folder, f\"{category}_boundary_points.gpkg\")\n",
    "    points_gdf.to_file(output_path, driver='GPKG')\n",
    "    print(f\"{category.capitalize()} data saved to {output_path}\")\n",
    "    \n",
    "    return points_gdf\n",
    "\n",
    "def calculate_accessibility(network, amenities_gdf, category, distance_600m=600, distance_15min=None):\n",
    "    \"\"\"\n",
    "    Calculate accessibility for a given category of amenities.\n",
    "    \n",
    "    Args:\n",
    "        network (pandana.Network): Pandana network object for accessibility analysis.\n",
    "        amenities_gdf (gpd.GeoDataFrame): GeoDataFrame containing amenities.\n",
    "        category (str): Category name for the amenities.\n",
    "        distance_600m (int): Distance in meters for calculating 600m accessibility.\n",
    "        distance_15min (float): Distance in meters for calculating 15-min accessibility.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with accessibility metrics added.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating accessibility for {category}...\")\n",
    "    \n",
    "    if distance_15min is None:\n",
    "        distance_15min = WALKING_SPEED_M_PER_MIN * 15  # Default 15-minute walking distance\n",
    "    \n",
    "    # Set POIs in the network\n",
    "    network.set_pois(category=category, maxdist=distance_15min, maxitems=len(amenities_gdf),\n",
    "                     x_col=amenities_gdf.geometry.x, y_col=amenities_gdf.geometry.y)\n",
    "    \n",
    "    # Calculate accessibility metrics\n",
    "    accessibility = {}\n",
    "    accessibility[f\"{category}_accessibility_600m\"] = network.nearest_pois(\n",
    "        distance=distance_600m, category=category, num_pois=len(amenities_gdf)\n",
    "    ).replace(distance_600m, np.nan).count(axis=1)\n",
    "    \n",
    "    accessibility[f\"{category}_accessibility_15min\"] = network.nearest_pois(\n",
    "        distance=distance_15min, category=category, num_pois=len(amenities_gdf)\n",
    "    ).replace(distance_15min, np.nan).count(axis=1)\n",
    "    \n",
    "    print(f\"Accessibility for {category} calculated.\")\n",
    "    return pd.DataFrame(accessibility)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    city = \"WÃ¼rzburg, Germany\"\n",
    "    output_folder = \"processed_green_blue_spaces\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Define categories and tags for green and blue spaces\n",
    "    amenities = {\n",
    "        \"parks\": {\"leisure\": \"park\"},\n",
    "        \"water\": {\"natural\": [\"water\", \"riverbank\"], \"waterway\": \"river\"}\n",
    "    }\n",
    "    \n",
    "    # Download and process amenities\n",
    "    amenities_gdfs = {}\n",
    "    for category, tags in amenities.items():\n",
    "        amenities_gdfs[category] = download_and_process_amenity(category, tags, city, output_folder)\n",
    "    \n",
    "    # Initialize Pandana network (replace with your network setup)\n",
    "    # Example: network = pandana.Network(nodes_x, nodes_y, edges)\n",
    "    network = None  # Replace with actual network initialization\n",
    "    \n",
    "    # Calculate accessibility for each category\n",
    "    for category, gdf in amenities_gdfs.items():\n",
    "        if network is not None:\n",
    "            accessibility_metrics = calculate_accessibility(network, gdf, category)\n",
    "            print(accessibility_metrics.head())\n",
    "        else:\n",
    "            print(\"Network not initialized. Skipping accessibility calculations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbae05-ea1e-406d-a5fd-d6197c058ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDVI\n",
    "%%time \n",
    "stats = rasterstats.zonal_stats(\n",
    "    func_data.geometry, \n",
    "    raster=folder + \"ndvi/2017_NDVI.tif\", \n",
    "    stats=['min', 'max', 'median', 'mean']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394fa55-c5a0-4b2f-827e-c801e9a751b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsdf = pd.DataFrame(stats, index=func_data.index)\n",
    "statsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2196f84-9a0b-4db4-8d65-3e3e24900db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_data['ndvi_min'] = statsdf['min']\n",
    "func_data['ndvi_max'] = statsdf['max']\n",
    "func_data['ndvi_mean'] = statsdf['mean']\n",
    "func_data['ndvi_median'] = statsdf['median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a41c3-d0fa-48ee-9b03-2234fe2d7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar processing for lst levels\n",
    "# Perform zonal stats for LST (Land Surface Temperature)\n",
    "lst_stats = zonal_stats(data.geometry, raster=folder + \"/sat/LST_S_2023-06-01_2023-09-01.tif\",\n",
    "                        stats=['max'])\n",
    "lst_statsdf = pd.DataFrame(lst_stats)\n",
    "\n",
    "# Rename columns in lst_statsdf to prevent overlap\n",
    "lst_statsdf.columns = [f'lst_{col}' for col in lst_statsdf.columns]\n",
    "\n",
    "# Join the dataframes\n",
    "func_data = func_data.join(lst_statsdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385c063-b5f6-4a36-9d80-0ed0c631666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to the input files\n",
    "tessellation_file = \"path_to_tessellation.gpkg\"  \n",
    "noise_file = \"path_to_noise_layer.tif\"  # Noise raster layer (L_den > 55 dB(A))\n",
    "\n",
    "# Load tessellation cells\n",
    "tessellation_gdf = gpd.read_file(tessellation_file)\n",
    "\n",
    "# Ensure the CRS of tessellation matches the noise raster\n",
    "tessellation_gdf = tessellation_gdf.to_crs(epsg=32634)  # Update to match CRS of noise raster\n",
    "\n",
    "# Perform zonal statistics to extract maximum noise levels\n",
    "noise_stats = zonal_stats(\n",
    "    tessellation_gdf.geometry,\n",
    "    noise_file,\n",
    "    stats=['max']\n",
    ")\n",
    "\n",
    "# Convert the results into a DataFrame and rename columns\n",
    "noise_stats_df = pd.DataFrame(noise_stats)\n",
    "noise_stats_df.columns = [f'noise_{col}' for col in noise_stats_df.columns]\n",
    "\n",
    "# Join noise data with tessellation GeoDataFrame\n",
    "tessellation_gdf = tessellation_gdf.join(noise_stats_df)\n",
    "\n",
    "# Save the updated tessellation with noise data\n",
    "output_file = \"tessellation_with_noise.gpkg\"\n",
    "tessellation_gdf.to_file(output_file, driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e6bdc-2493-4caa-8c04-20000261f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching data from the sensor.community API\n",
    "url = \"https://data.sensor.community/airrohr/v1/filter/country=DE\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Failed to fetch data: Status code\", response.status_code)\n",
    "    data = []\n",
    "\n",
    "# Convert to DataFrame\n",
    "sensor_data = pd.DataFrame(data)\n",
    "\n",
    "# Filter for stations in WÃ¼rzburg (approximate coordinates)\n",
    "wurzburg_lat_range = (49.75, 49.85)\n",
    "wurzburg_lon_range = (9.90, 10.00)\n",
    "\n",
    "# Extracting location information and filtering\n",
    "sensor_data['latitude'] = sensor_data['location'].apply(lambda x: float(x['latitude']))\n",
    "sensor_data['longitude'] = sensor_data['location'].apply(lambda x: float(x['longitude']))\n",
    "wurzburg_data = sensor_data[\n",
    "    (sensor_data['latitude'].between(*wurzburg_lat_range)) & \n",
    "    (sensor_data['longitude'].between(*wurzburg_lon_range))\n",
    "]\n",
    "\n",
    "# Extract PM2.5 and PM10 measurements\n",
    "def extract_pm_values(sensordatavalues):\n",
    "    pm_values = {'PM2.5': None, 'PM10': None}\n",
    "    for record in sensordatavalues:\n",
    "        if record['value_type'] == 'P1':  # PM10\n",
    "            pm_values['PM10'] = float(record['value'])\n",
    "        elif record['value_type'] == 'P2':  # PM2.5\n",
    "            pm_values['PM2.5'] = float(record['value'])\n",
    "    return pm_values\n",
    "\n",
    "wurzburg_data['PM_values'] = wurzburg_data['sensordatavalues'].apply(extract_pm_values)\n",
    "\n",
    "# Displaying the filtered data\n",
    "wurzburg_data[['timestamp', 'PM_values']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aec94d-1b64-4207-a07d-bd1973a0e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Convert PM data to a GeoDataFrame\n",
    "pm_data = pd.DataFrame(wurzburg_data['PM_values'].tolist())\n",
    "pm_data['latitude'] = wurzburg_data['latitude']\n",
    "pm_data['longitude'] = wurzburg_data['longitude']\n",
    "pm_data['geometry'] = [Point(lon, lat) for lon, lat in zip(pm_data['longitude'], pm_data['latitude'])]\n",
    "pm_geo_data = gpd.GeoDataFrame(pm_data, geometry='geometry')\n",
    "\n",
    "# Set the CRS for pm_geo_data to match the land_use GeoDataFrame\n",
    "# Replace 'EPSG_Code' with the correct EPSG code of your land_use data\n",
    "pm_geo_data.set_crs(epsg=32634, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ae709-8e2e-4469-9fd0-882266064e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the spatial join\n",
    "pm_land_use = gpd.sjoin(land_use, pm_geo_data, how='left', op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274afd5-230b-4ece-994d-e2d3ad6ac709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate PM values by land use class\n",
    "# Replace 'land_use_class_column' with the actual column name for land use classes\n",
    "pm_aggregated = pm_land_use.groupby('class_2018').agg({'PM2.5': 'mean', 'PM10': 'mean'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace05400-46e1-4ff1-a8eb-360acf5dcd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the aggregated PM data into combined_data\n",
    "# Ensure the land use class column name matches in both dataframes\n",
    "combined_data = combined_data.merge(pm_aggregated, on='class_2018', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c84d07-c621-4b25-bab4-3a1c126e59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "stadtbezirke_hauptwohnsitz_altersgruppen = '...stadtbezirke_hauptwohnsitz_altersgruppen.csv'  # Update the filename if necessary\n",
    "haushalte_df = gpd.read_file(stadtbezirke_hauptwohnsitz_altersgruppen)\n",
    "\n",
    "\n",
    "stadtbezirke_haushalte_durchschnittsgroesse = 'stadtbezirke_haushalte_durchschnittsgroesse.csv'  # Update the filename if necessary\n",
    "hauptwohnsitz_df = gpd.read_file(stadtbezirke_haushalte_durchschnittsgroesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cfcba6-1418-4c2d-b063-e5d8193fe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "# Convert 'Geo-Punkt' to geometry\n",
    "def extract_geometry(geo_punkt):\n",
    "    if geo_punkt:\n",
    "        lat, lon = map(float, geo_punkt.split(', '))\n",
    "        return Point(lon, lat)\n",
    "    return None\n",
    "\n",
    "hauptwohnsitz_df['geometry'] = hauptwohnsitz_df['Geo-Punkt'].apply(extract_geometry)\n",
    "hauptwohnsitz_gdf = gpd.GeoDataFrame(hauptwohnsitz_df, geometry='geometry')\n",
    "hauptwohnsitz_gdf.set_crs(epsg=4326, inplace=True)  # Assuming WGS 84 coordinate system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4851ec-1e6b-4a81-a070-94446529b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes on 'Stadtbezirk'\n",
    "merged_df = pd.merge(haushalte_df, hauptwohnsitz_gdf[['Stadtbezirk', 'geometry']], on='Stadtbezirk', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b76a2-d747-44ca-8618-e723d6489bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_census = gpd.read_file(\"census_2011_wue.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff17080-c645-48af-9427-7f56bd9c9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_census = econ_census.to_crs(nodes.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aec25e-2e78-4d81-9ec3-3863a473221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "network.set_pois(category = 'pois',\n",
    "                 maxdist = 1200,\n",
    "                 maxitems=10000,\n",
    "                 x_col = econ_census.geometry.x, \n",
    "                 y_col = econ_census.geometry.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce15d9-e8b0-4778-ba2a-d6854543c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nodes['pois'] = network.nearest_pois(distance = 1200,\n",
    "                               category = 'pois',\n",
    "                               num_pois = 10000,\n",
    "                               include_poi_ids = False).replace(1200, pd.NA).count(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485b08a-bc21-4ada-b561-6f18466fac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['pois'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03291c0-c7d6-41af-b5c2-3243a1e7f871",
   "metadata": {},
   "source": [
    "## 3. Cluster Analysis\n",
    "Cluster analysis leverages data on urban form, function, and their combination to identify homogeneous patterns of built form and activity. We employ a Gaussian Mixture Model (GMM), which provides a probabilistic approach to clustering. This method allows for overlapping clusters, capturing the complexity and subtlety of urban environments more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a30825-5e17-4d4c-ad83-e26ffe604514",
   "metadata": {},
   "source": [
    "### 3.1 Form\n",
    "Description and Python code for clustering based on form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92714d81-7bcd-4a3e-b49f-f92be12303c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "form = pd.read_parquet(folder + \"convolutions.pq\")\n",
    "form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8ee6c-fe22-49ea-b26d-bc40a30a2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data needs to be standardised. We use standard scaler.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee897e08-9d83-472f-abb9-bbead6b768a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb6ea1-b061-4ba1-82ce-7d33b53abf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[np.isnan(data)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479e3e5-3787-438a-9367-36f00c88bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Clustergram using GMM\n",
    "cgram = Clustergram(range(1, 30), method='gmm', covariance_type='full', random_state=42)\n",
    "cgram.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c101bb5-bf93-4763-b395-0d14ed3cc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting clustergram\n",
    "ax = cgram.plot(figsize=(20, 10), linewidth=0.5, cluster_style={\"edgecolor\": \"blue\", \"alpha\": 0.6}, size=1,\n",
    "                line_style={\"alpha\": 0.5})\n",
    "plt.title(\"Clustergram Using Gaussian Mixture Model\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Cluster Assignment Stability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06a88f-51f4-4ab7-b6bc-610eb2611a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters\n",
    "optimal_clusters = cgram.optimal_k_\n",
    "print(f\"Optimal number of clusters based on GMM: {optimal_clusters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f6504-d2f7-433f-8bb0-e39156f1c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign clusters to the data based on the optimal number\n",
    "gmm_labels = cgram.labels_[optimal_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95fe53-25ec-4a7e-8525-b925d585fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the cluster labels to tessellation data\n",
    "tess = gpd.read_parquet(folder + \"tess.pq\")\n",
    "tess['clusters_form'] = gmm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3e08f-665d-47c8-9b85-0ed5432be8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tessellation with clusters\n",
    "tess.to_file(folder + \"tess_with_gmm_clusters_form.gpkg\", driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3634c-e6e6-417a-9c1b-66562bd0b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spatial distribution of clusters\n",
    "ax = tess.plot('clusters_form', categorical=True, legend=True, figsize=(20, 20), cmap='tab20')\n",
    "ax.set_axis_off()\n",
    "plt.title(f\"Spatial Clusters (GMM, {optimal_clusters} Clusters)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5979065-8eab-4958-8a1d-1a90c62f8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tess.set_geometry('buildings').plot('clusters_form', categorical=True, legend=True, figsize=(20, 20), cmap='tab20')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502ecc4-901b-4173-b869-41cae1280fd1",
   "metadata": {},
   "source": [
    "## Generate Spatial Signatures\n",
    "As the final step, we create geometries representing spatial signatures by combining contiguous tessellation cells that belong to the same cluster. To efficiently dissolve the large number of polygons involved, we utilize dask-geopandas for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8835cf-e4f0-4af2-9dfc-291330684b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(LocalCluster(n_workers=16))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d90b7-0064-4253-b693-635d97259320",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = gpd.read_parquet(folder + \"tess.pq\", columns=['tessellation']).rename_geometry(\"geometry\")\n",
    "clusters = pd.read_csv(folder + \"FINAL_cluster_labels.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ead841-f75e-4144-a93a-514c59c36672",
   "metadata": {},
   "source": [
    "dask_dissolve mimics the behaviour of geopandas.dissolve, just based on parallel implementation using dask-geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe25242-9d86-474a-ad78-ff71dfc5208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dask_geopandas.from_geopandas(tess.sort_values('cluster'), npartitions=64)\n",
    "final = dask_dissolve(ddf, by='cluster').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae0956-890c-454e-873d-5cc4ba3ca0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.plot('cluster', categorical=True, figsize=(20, 20), cmap='tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04843723-e281-4467-8987-b1c382503207",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b996ba-8c70-4d2f-99cf-3b19094a4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "final = gpd.read_parquet(folder + \"signatures.pq\")\n",
    "enc = gpd.read_parquet(folder + \"enclosures.pq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c899d7-171a-4a43-a606-b29b35ea9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up styling\n",
    "sns.set(context=\"paper\", style=\"ticks\", rc={'patch.force_edgecolor': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83f786-d290-4c6a-96c2-1fb10f1662fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create colormap\n",
    "cmap = ugg.get_colormap(final.cluster.nunique(), randomize=False)\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ff4a9-5a30-419c-b757-e37b2e71cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate symbology dynamically\n",
    "cols = cmap.colors\n",
    "symbology = {i: cols[i % len(cols)] for i in range(final.cluster.nunique())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5dd00-1d14-4661-8fb5-e2e71366b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder names for clusters\n",
    "names = {i: f\"Cluster {i}\" for i in range(final.cluster.nunique())}\n",
    "\n",
    "# Prepare data for plotting\n",
    "df = final.set_crs(enc.crs).to_crs(3857)\n",
    "token = \"\"\n",
    "\n",
    "# Plot clusters\n",
    "ax = df.plot(\n",
    "    color=df['cluster'].map(symbology),\n",
    "    figsize=(20, 20),\n",
    "    zorder=1,\n",
    "    linewidth=0.3,\n",
    "    edgecolor='w',\n",
    "    alpha=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d944e6-a8a6-4427-be2c-6b7aadce72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add basemaps\n",
    "contextily.add_basemap(ax, crs=df.crs, source=ugg.get_tiles('roads', token), zorder=2, alpha=0.3)\n",
    "contextily.add_basemap(ax, crs=df.crs, source=ugg.get_tiles('labels', token), zorder=3, alpha=1)\n",
    "contextily.add_basemap(ax, crs=df.crs, source=ugg.get_tiles('background', token), zorder=-1, alpha=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f936f5-0dda-478a-b9f7-6a8eee4e76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scalebar\n",
    "scalebar = ScaleBar(\n",
    "    dx=1,\n",
    "    color=ugg.COLORS[0],\n",
    "    location='lower right',\n",
    "    height_fraction=0.002,\n",
    "    pad=0.5,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.add_artist(scalebar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8c2e-d7ec-415f-9828-7ebcfdbede78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add north arrow\n",
    "ugg.north_arrow(\n",
    "    plt.gcf(),\n",
    "    ax,\n",
    "    0,\n",
    "    size=0.026,\n",
    "    linewidth=1,\n",
    "    color=ugg.COLORS[0],\n",
    "    loc=\"upper left\",\n",
    "    pad=0.002,\n",
    "    alpha=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4266eb-2c01-47a7-920c-22e14df35164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add legend\n",
    "custom_points = [\n",
    "    Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=10, color=color) \n",
    "    for color in symbology.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8aaf45-ee60-4561-b99a-7eb4539d286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_points = ax.legend(\n",
    "    custom_points,\n",
    "    [f\"{name}\" for name in names.values()],\n",
    "    loc='upper right',\n",
    "    frameon=True\n",
    ")\n",
    "ax.add_artist(leg_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95824e7-2541-4ad5-8e01-e654a00baf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final map\n",
    "plt.savefig(folder + \"signatures.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd07d52-8d15-4ef5-bd23-42c1da84f2e6",
   "metadata": {},
   "source": [
    "## 4. Contextual Purposive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca98e03-8c32-4ce1-bea3-3af80d421757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b676dbe-0d47-4a06-815a-e55a28ea7199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f9c2b-678f-4e00-bdb1-c73c4acb6a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e21e05-ee7c-48bb-a99d-4fb2c2e1d045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
